{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc4d1e5",
   "metadata": {},
   "source": [
    "# ===========================\n",
    "# üîÑ MODEL C·∫¢I TI·∫æN - NH·∫¨N D·∫†NG 3 H√åNH: TR√íN, VU√îNG, TAM GI√ÅC\n",
    "# ===========================\n",
    "# Model n√†y ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ nh·∫≠n d·∫°ng ch√≠nh x√°c h∆°n v·ªõi ·∫£nh nhi·ªÅu m√†u v√† nhi·ªÅu h√¨nh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5cb99bb-7816-457f-b498-9c408f8901e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H√†m detect_and_classify_shapes_improved ƒë√£ s·∫µn s√†ng!\n",
      "‚úÖ H√†m create_improved_gradio_interface() ƒë√£ s·∫µn s√†ng!\n",
      "üí° M·∫∑c ƒë·ªãnh, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông chuy·ªÉn ·∫£nh sang grayscale (·∫£nh ƒëen tr·∫Øng) ƒë·ªÉ x·ª≠ l√Ω!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ H√ÄM NH·∫¨N D·∫†NG C·∫¢I THI·ªÜN V·ªöI PREPROCESSING M·ªöI\n",
    "# ===========================\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def detect_and_classify_shapes_improved(image_path_or_array, model, img_size=None, min_area=100, use_grayscale=True, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Ph√°t hi·ªán v√† ph√¢n lo·∫°i nhi·ªÅu h√¨nh v·ªõi preprocessing c·∫£i thi·ªán\n",
    "    M·∫∑c ƒë·ªãnh t·ª± ƒë·ªông chuy·ªÉn ·∫£nh sang grayscale ƒë·ªÉ x·ª≠ l√Ω\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_array: ƒê∆∞·ªùng d·∫´n ·∫£nh ho·∫∑c numpy array\n",
    "        model: Model ƒë√£ train (3 classes: circle, square, triangle)\n",
    "        img_size: K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o (None = t·ª± ƒë·ªông detect)\n",
    "        min_area: Di·ªán t√≠ch t·ªëi thi·ªÉu (m·∫∑c ƒë·ªãnh 100, gi·∫£m ƒë·ªÉ ph√°t hi·ªán h√¨nh nh·ªè h∆°n)\n",
    "        use_grayscale: N·∫øu True, chuy·ªÉn ·∫£nh sang grayscale tr∆∞·ªõc khi x·ª≠ l√Ω (m·∫∑c ƒë·ªãnh True)\n",
    "        confidence_threshold: Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y (m·∫∑c ƒë·ªãnh 0.5, gi·∫£m ƒë·ªÉ ph√°t hi·ªán nhi·ªÅu h√¨nh h∆°n)\n",
    "    \n",
    "    Returns:\n",
    "        result_image: ·∫¢nh ƒë√£ v·∫Ω bounding boxes v√† labels\n",
    "        detections: List c√°c detection\n",
    "    \"\"\"\n",
    "    # Detect model input shape\n",
    "    model_input_shape = model.input_shape[1:]  # B·ªè qua batch dimension\n",
    "    if len(model_input_shape) == 3:\n",
    "        # Format: (height, width, channels)\n",
    "        model_img_size = model_input_shape[0]\n",
    "        model_channels = model_input_shape[2]\n",
    "    elif len(model_input_shape) == 2:\n",
    "        # Format: (height, width) - grayscale\n",
    "        model_img_size = model_input_shape[0]\n",
    "        model_channels = 1\n",
    "    else:\n",
    "        # Fallback\n",
    "        model_img_size = 200  # Model pickle th∆∞·ªùng l√† 200x200\n",
    "        model_channels = 1   # Grayscale\n",
    "    \n",
    "    if img_size is None:\n",
    "        img_size = model_img_size\n",
    "    \n",
    "    # N·∫øu model y√™u c·∫ßu grayscale, t·ª± ƒë·ªông chuy·ªÉn sang grayscale\n",
    "    if model_channels == 1:\n",
    "        use_grayscale = True\n",
    "        print(\"üñºÔ∏è Model y√™u c·∫ßu ·∫£nh grayscale, t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi...\")\n",
    "    \n",
    "    print(f\"üîç Model input: {img_size}x{img_size}x{model_channels}\")\n",
    "    \n",
    "    # Ph√°t hi·ªán contours v·ªõi preprocessing c·∫£i thi·ªán (m·∫∑c ƒë·ªãnh d√πng grayscale)\n",
    "    contours, img, mask = detect_shapes_improved(image_path_or_array, min_area=min_area, use_grayscale=use_grayscale)\n",
    "    \n",
    "    print(f\"üîç ƒê√£ t√¨m th·∫•y {len(contours)} contours\")\n",
    "    \n",
    "    # L·∫•y ·∫£nh grayscale ƒë·ªÉ d√πng cho model prediction n·∫øu model y√™u c·∫ßu\n",
    "    if model_channels == 1:\n",
    "        img_gray = convert_to_grayscale(img)\n",
    "    else:\n",
    "        img_gray = None\n",
    "    \n",
    "    result_img = img.copy()\n",
    "    \n",
    "    # T·ª± ƒë·ªông detect s·ªë classes t·ª´ model output shape\n",
    "    # Test predict v·ªõi m·ªôt ·∫£nh dummy ƒë·ªÉ bi·∫øt s·ªë classes\n",
    "    try:\n",
    "        dummy_input = np.zeros((1, img_size, img_size, model_channels), dtype='float32')\n",
    "        dummy_pred = model.predict(dummy_input, verbose=0)\n",
    "        num_classes = dummy_pred.shape[1] if len(dummy_pred.shape) > 1 else len(dummy_pred[0])\n",
    "    except:\n",
    "        # Fallback: gi·∫£ ƒë·ªãnh 3 classes (model m·ªõi train v·ªõi 3 classes: circle, square, triangle)\n",
    "        num_classes = 3\n",
    "    \n",
    "    # Labels d·ª±a tr√™n s·ªë classes\n",
    "    if num_classes == 3:\n",
    "        labels = ['circle', 'square', 'triangle']\n",
    "        labels_vn = ['Hinh tron', 'Hinh vuong', 'Tam giac']\n",
    "    else:\n",
    "        # T·∫°o labels m·∫∑c ƒë·ªãnh\n",
    "        labels = [f'class_{i}' for i in range(num_classes)]\n",
    "        labels_vn = [f'L·ªõp {i}' for i in range(num_classes)]\n",
    "    \n",
    "    print(f\"üîç Model c√≥ {num_classes} classes: {labels}\")\n",
    "    print(f\"üìä Ng∆∞·ª°ng confidence: {confidence_threshold:.2f}, Min area: {min_area}\")\n",
    "    \n",
    "    detections = []\n",
    "    total_predictions = 0\n",
    "    low_confidence_count = 0\n",
    "    \n",
    "    # Duy·ªát qua contours\n",
    "    for contour in contours:\n",
    "        # L·∫•y bounding box v·ªõi padding\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        padding = max(5, int(min(w, h) * 0.1))  # Padding ƒë·ªông\n",
    "        \n",
    "        x = max(0, x - padding)\n",
    "        y = max(0, y - padding)\n",
    "        w = min(img.shape[1] - x, w + 2 * padding)\n",
    "        h = min(img.shape[0] - y, h + 2 * padding)\n",
    "        \n",
    "        # Crop ROI - s·ª≠ d·ª•ng ·∫£nh grayscale n·∫øu model y√™u c·∫ßu\n",
    "        if model_channels == 1 and img_gray is not None:\n",
    "            # S·ª≠ d·ª•ng ·∫£nh grayscale ƒë·ªÉ crop ROI (t·ªët h∆°n cho model ƒë√£ train v·ªõi grayscale)\n",
    "            roi_gray_crop = img_gray[y:y+h, x:x+w]\n",
    "            if roi_gray_crop.size == 0:\n",
    "                continue\n",
    "            roi_resized = cv2.resize(roi_gray_crop, (img_size, img_size))\n",
    "            roi_normalized = roi_resized.reshape(1, img_size, img_size, 1).astype('float32') / 255.0\n",
    "        else:\n",
    "            # S·ª≠ d·ª•ng ·∫£nh m√†u\n",
    "            roi_bgr = img[y:y+h, x:x+w]\n",
    "            if roi_bgr.size == 0:\n",
    "                continue\n",
    "            # X·ª≠ l√Ω ROI theo model\n",
    "            if model_channels == 3:\n",
    "                roi_rgb = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2RGB)\n",
    "                roi_resized = cv2.resize(roi_rgb, (img_size, img_size))\n",
    "                roi_normalized = roi_resized.reshape(1, img_size, img_size, 3).astype('float32') / 255.0\n",
    "            else:\n",
    "                # Grayscale (1 channel) t·ª´ ·∫£nh m√†u\n",
    "                roi_gray = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)\n",
    "                roi_resized = cv2.resize(roi_gray, (img_size, img_size))\n",
    "                roi_normalized = roi_resized.reshape(1, img_size, img_size, 1).astype('float32') / 255.0\n",
    "        \n",
    "        # D·ª± ƒëo√°n\n",
    "        prediction = model.predict(roi_normalized, verbose=0)\n",
    "        \n",
    "        # X·ª≠ l√Ω output shape kh√°c nhau\n",
    "        if len(prediction.shape) == 2:\n",
    "            pred_array = prediction[0]\n",
    "        else:\n",
    "            pred_array = prediction[0] if len(prediction) > 0 else prediction\n",
    "        \n",
    "        class_idx = np.argmax(pred_array)\n",
    "        confidence = float(pred_array[class_idx])\n",
    "        total_predictions += 1\n",
    "        \n",
    "        # Ki·ªÉm tra class_idx c√≥ h·ª£p l·ªá kh√¥ng\n",
    "        if class_idx >= len(labels):\n",
    "            print(f\"‚ö†Ô∏è Warning: class_idx {class_idx} >= s·ªë classes {len(labels)}, b·ªè qua\")\n",
    "            continue\n",
    "        \n",
    "        # Hi·ªÉn th·ªã n·∫øu confidence > threshold (m·∫∑c ƒë·ªãnh 0.5, c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)\n",
    "        if confidence > confidence_threshold:\n",
    "            # M√†u s·∫Øc kh√°c nhau cho m·ªói class (3 classes: circle, square, triangle)\n",
    "            colors = {\n",
    "                0: (0, 255, 0),    # Circle - Xanh l√°\n",
    "                1: (255, 0, 0),    # Square - Xanh d∆∞∆°ng\n",
    "                2: (0, 0, 255)     # Triangle - ƒê·ªè\n",
    "            }\n",
    "            color = colors.get(class_idx, (128, 128, 128))  # M√†u x√°m m·∫∑c ƒë·ªãnh\n",
    "            \n",
    "            # V·∫Ω bounding box\n",
    "            cv2.rectangle(result_img, (x, y), (x + w, y + h), color, 3)\n",
    "            \n",
    "            # V·∫Ω contour\n",
    "            cv2.drawContours(result_img, [contour], -1, color, 2)\n",
    "            \n",
    "            # V·∫Ω label v·ªõi background\n",
    "            label_text = f\"{labels_vn[class_idx]} {confidence:.2f}\"\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2\n",
    "            )\n",
    "            \n",
    "            # Background cho text\n",
    "            cv2.rectangle(\n",
    "                result_img,\n",
    "                (x, y - text_height - 15),\n",
    "                (x + text_width + 10, y),\n",
    "                color,\n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            # Text\n",
    "            cv2.putText(\n",
    "                result_img,\n",
    "                label_text,\n",
    "                (x + 5, y - 8),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (255, 255, 255),\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            detections.append({\n",
    "                'x': int(x),\n",
    "                'y': int(y),\n",
    "                'w': int(w),\n",
    "                'h': int(h),\n",
    "                'class': labels[class_idx],\n",
    "                'class_vn': labels_vn[class_idx],\n",
    "                'confidence': float(confidence)\n",
    "            })\n",
    "        else:\n",
    "            low_confidence_count += 1\n",
    "            # In th√¥ng tin v·ªÅ c√°c h√¨nh c√≥ confidence th·∫•p (ƒë·ªÉ debug)\n",
    "            if confidence > 0.3:  # Ch·ªâ in nh·ªØng h√¨nh c√≥ confidence > 30%\n",
    "                print(f\"  ‚ö†Ô∏è H√¨nh b·ªã b·ªè qua: {labels[class_idx]} (confidence: {confidence:.2%} < {confidence_threshold:.2%})\")\n",
    "    \n",
    "    print(f\"\\nüìä T·ªïng k·∫øt: {total_predictions} h√¨nh ƒë∆∞·ª£c d·ª± ƒëo√°n, {len(detections)} h√¨nh ƒë∆∞·ª£c ch·∫•p nh·∫≠n (confidence > {confidence_threshold:.2%})\")\n",
    "    if low_confidence_count > 0:\n",
    "        print(f\"   {low_confidence_count} h√¨nh b·ªã b·ªè qua do confidence th·∫•p\")\n",
    "    \n",
    "    return result_img, detections\n",
    "\n",
    "print(\"‚úÖ H√†m detect_and_classify_shapes_improved ƒë√£ s·∫µn s√†ng!\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ T·∫†O GIAO DI·ªÜN GRADIO C·∫¢I THI·ªÜN\n",
    "# ===========================\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Bi·∫øn global ƒë·ªÉ l∆∞u model\n",
    "_loaded_model = None\n",
    "\n",
    "def load_model_for_gradio(model_path=None):\n",
    "    \"\"\"\n",
    "    Load model cho Gradio interface\n",
    "    \n",
    "    Args:\n",
    "        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model (.h5). N·∫øu None, s·∫Ω th·ª≠ load c√°c model m·∫∑c ƒë·ªãnh\n",
    "    \"\"\"\n",
    "    global _loaded_model\n",
    "    \n",
    "    if _loaded_model is not None:\n",
    "        return _loaded_model\n",
    "    \n",
    "    # Danh s√°ch c√°c model m·∫∑c ƒë·ªãnh ƒë·ªÉ th·ª≠\n",
    "    default_models = [\n",
    "        \"shapes_3class_improved.h5\"\n",
    "    ]\n",
    "    \n",
    "    if model_path is None:\n",
    "        # Th·ª≠ load c√°c model m·∫∑c ƒë·ªãnh\n",
    "        for model_file in default_models:\n",
    "            try:\n",
    "                _loaded_model = load_model(model_file)\n",
    "                print(f\"‚úÖ ƒê√£ load model: {model_file}\")\n",
    "                return _loaded_model\n",
    "            except:\n",
    "                continue\n",
    "        raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y model n√†o. Vui l√≤ng ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n model.\")\n",
    "    else:\n",
    "        _loaded_model = load_model(model_path)\n",
    "        print(f\"‚úÖ ƒê√£ load model: {model_path}\")\n",
    "        return _loaded_model\n",
    "\n",
    "def predict_shape(image, model_path=None, use_grayscale=True):\n",
    "    \"\"\"\n",
    "    H√†m x·ª≠ l√Ω ·∫£nh cho Gradio interface\n",
    "    M·∫∑c ƒë·ªãnh t·ª± ƒë·ªông chuy·ªÉn ·∫£nh sang grayscale\n",
    "    \n",
    "    Args:\n",
    "        image: ·∫¢nh t·ª´ Gradio (numpy array)\n",
    "        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn model (n·∫øu c·∫ßn load m·ªõi)\n",
    "        use_grayscale: C√≥ chuy·ªÉn sang grayscale kh√¥ng (m·∫∑c ƒë·ªãnh True)\n",
    "    \n",
    "    Returns:\n",
    "        ·∫¢nh k·∫øt qu·∫£ v·ªõi bounding boxes v√† labels, th√¥ng tin k·∫øt qu·∫£\n",
    "    \"\"\"\n",
    "    global _loaded_model\n",
    "    \n",
    "    if image is None:\n",
    "        return None, \"Vui l√≤ng upload ·∫£nh\"\n",
    "    \n",
    "    try:\n",
    "        # Load model n·∫øu ch∆∞a c√≥\n",
    "        if _loaded_model is None:\n",
    "            load_model_for_gradio(model_path)\n",
    "        \n",
    "        # Chuy·ªÉn ƒë·ªïi ·∫£nh t·ª´ RGB sang BGR (OpenCV format)\n",
    "        if len(image.shape) == 3:\n",
    "            image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            image_bgr = image\n",
    "        \n",
    "        # Nh·∫≠n d·∫°ng h√¨nh d·∫°ng (m·∫∑c ƒë·ªãnh d√πng grayscale)\n",
    "        # Gi·∫£m min_area v√† confidence_threshold ƒë·ªÉ ph√°t hi·ªán nhi·ªÅu h√¨nh h∆°n\n",
    "        result_img, detections = detect_and_classify_shapes_improved(\n",
    "            image_bgr, \n",
    "            _loaded_model, \n",
    "            min_area=50,  # Gi·∫£m min_area ƒë·ªÉ ph√°t hi·ªán h√¨nh nh·ªè h∆°n\n",
    "            use_grayscale=use_grayscale,\n",
    "            confidence_threshold=0.4  # Gi·∫£m ng∆∞·ª°ng confidence ƒë·ªÉ ph√°t hi·ªán nhi·ªÅu h√¨nh h∆°n\n",
    "        )\n",
    "        \n",
    "        # Chuy·ªÉn l·∫°i sang RGB ƒë·ªÉ hi·ªÉn th·ªã trong Gradio\n",
    "        result_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # T·∫°o th√¥ng tin k·∫øt qu·∫£\n",
    "        info_text = f\"ƒê√£ ph√°t hi·ªán {len(detections)} h√¨nh:\\n\"\n",
    "        for i, det in enumerate(detections, 1):\n",
    "            info_text += f\"{i}. {det['class_vn']} (ƒë·ªô tin c·∫≠y: {det['confidence']:.2%})\\n\"\n",
    "        \n",
    "        if len(detections) == 0:\n",
    "            info_text = \"Kh√¥ng ph√°t hi·ªán h√¨nh n√†o (ƒë·ªô tin c·∫≠y < 60%)\"\n",
    "        \n",
    "        return result_rgb, info_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"L·ªói: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return None, error_msg\n",
    "\n",
    "def create_improved_gradio_interface(model_path=None):\n",
    "    \"\"\"\n",
    "    T·∫°o giao di·ªán Gradio c·∫£i thi·ªán cho nh·∫≠n d·∫°ng h√¨nh d·∫°ng\n",
    "    M·∫∑c ƒë·ªãnh t·ª± ƒë·ªông chuy·ªÉn ·∫£nh sang grayscale\n",
    "    \n",
    "    Args:\n",
    "        model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model (.h5). N·∫øu None, s·∫Ω t·ª± ƒë·ªông t√¨m\n",
    "    \n",
    "    Returns:\n",
    "        Gradio Interface object\n",
    "    \"\"\"\n",
    "    # Load model tr∆∞·ªõc\n",
    "    try:\n",
    "        load_model_for_gradio(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o: {e}\")\n",
    "        print(\"üí° B·∫°n c√≥ th·ªÉ load model sau khi ch·∫°y interface\")\n",
    "    \n",
    "    # T·∫°o giao di·ªán\n",
    "    with gr.Blocks(title=\"Nh·∫≠n d·∫°ng h√¨nh d·∫°ng c·∫£i thi·ªán\") as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # üéØ Nh·∫≠n d·∫°ng H√¨nh D·∫°ng\n",
    "            ### Upload ·∫£nh ƒë·ªÉ nh·∫≠n d·∫°ng c√°c h√¨nh: Tr√≤n, Vu√¥ng, Tam gi√°c\n",
    "            \n",
    "            **T√≠nh nƒÉng:**\n",
    "            - ‚úÖ Ph√°t hi·ªán nhi·ªÅu h√¨nh trong m·ªôt ·∫£nh\n",
    "            - ‚úÖ Hi·ªÉn th·ªã ƒë·ªô tin c·∫≠y cho m·ªói h√¨nh\n",
    "            - ‚úÖ Model ƒë√£ train v·ªõi 3 classes: Circle, Square, Triangle\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image_input = gr.Image(\n",
    "                    label=\"Upload ·∫£nh\",\n",
    "                    type=\"numpy\",\n",
    "                    height=400\n",
    "                )\n",
    "                    \n",
    "                predict_btn = gr.Button(\"üîç Nh·∫≠n d·∫°ng\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            with gr.Column():\n",
    "                image_output = gr.Image(\n",
    "                    label=\"K·∫øt qu·∫£ nh·∫≠n d·∫°ng\",\n",
    "                    type=\"numpy\",\n",
    "                    height=400\n",
    "                )\n",
    "                info_output = gr.Textbox(\n",
    "                    label=\"Th√¥ng tin\",\n",
    "                    lines=10,\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        # X·ª≠ l√Ω s·ª± ki·ªán\n",
    "        predict_btn.click(\n",
    "            fn=lambda img, use_gray: predict_shape(img, None, use_gray),\n",
    "            inputs=[image_input],\n",
    "            outputs=[image_output, info_output]\n",
    "        )\n",
    "        \n",
    "        # Auto predict khi upload ·∫£nh\n",
    "        image_input.change(\n",
    "            fn=lambda img, use_gray: predict_shape(img, None, use_gray),\n",
    "            inputs=[image_input],\n",
    "            outputs=[image_output, info_output]\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### üí° H∆∞·ªõng d·∫´n:\n",
    "            1. Upload ·∫£nh ch·ª©a c√°c h√¨nh d·∫°ng (tr√≤n, vu√¥ng, tam gi√°c)\n",
    "            2. H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông chuy·ªÉn sang grayscale (·∫£nh ƒëen tr·∫Øng) ƒë·ªÉ x·ª≠ l√Ω\n",
    "            3. Xem k·∫øt qu·∫£ nh·∫≠n d·∫°ng v·ªõi bounding boxes v√† ƒë·ªô tin c·∫≠y\n",
    "            \n",
    "            **L∆∞u √Ω:** Model ch·ªâ nh·∫≠n d·∫°ng 3 h√¨nh: H√¨nh tr√≤n, H√¨nh vu√¥ng, Tam gi√°c\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "print(\"‚úÖ H√†m create_improved_gradio_interface() ƒë√£ s·∫µn s√†ng!\")\n",
    "print(\"üí° M·∫∑c ƒë·ªãnh, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông chuy·ªÉn ·∫£nh sang grayscale (·∫£nh ƒëen tr·∫Øng) ƒë·ªÉ x·ª≠ l√Ω!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ff4a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:1052: UserWarning: Expected 2 arguments for function <function create_improved_gradio_interface.<locals>.<lambda> at 0x000002074C705E40>, received 1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:1056: UserWarning: Expected at least 2 arguments for function <function create_improved_gradio_interface.<locals>.<lambda> at 0x000002074C705E40>, received 1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:1052: UserWarning: Expected 2 arguments for function <function create_improved_gradio_interface.<locals>.<lambda> at 0x000002074C87EAC0>, received 1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:1056: UserWarning: Expected at least 2 arguments for function <function create_improved_gradio_interface.<locals>.<lambda> at 0x000002074C87EAC0>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load model: shapes_3class_improved.h5\n",
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Giao di·ªán Gradio ƒë√£ ch·∫°y!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastapi\\routing.py\", line 111, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastapi\\routing.py\", line 391, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastapi\\routing.py\", line 290, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 479, in wait_for\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000020747BF9150 [unset]> is bound to a different event loop\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\helpers.py:1060: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Model y√™u c·∫ßu ·∫£nh grayscale, t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi...\n",
      "üîç Model input: 200x200x1\n",
      "üîç ƒê√£ t√¨m th·∫•y 1 contours\n",
      "üîç Model c√≥ 3 classes: ['circle', 'square', 'triangle']\n",
      "üìä Ng∆∞·ª°ng confidence: 0.40, Min area: 50\n",
      "\n",
      "üìä T·ªïng k·∫øt: 1 h√¨nh ƒë∆∞·ª£c d·ª± ƒëo√°n, 1 h√¨nh ƒë∆∞·ª£c ch·∫•p nh·∫≠n (confidence > 40.00%)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ CH·∫†Y GIAO DI·ªÜN GRADIO C·∫¢I THI·ªÜN\n",
    "# ===========================\n",
    "\n",
    "# T·∫°o v√† launch giao di·ªán\n",
    "demo_improved = create_improved_gradio_interface()\n",
    "\n",
    "# Launch v·ªõi c√°c t√πy ch·ªçn ƒë·ªÉ tr√°nh l·ªói event loop\n",
    "demo_improved.launch()\n",
    "\n",
    "print(\"\\n‚úÖ Giao di·ªán Gradio ƒë√£ ch·∫°y!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a9032-64e1-40f7-8761-2df805648c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
